{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports loaded!\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DimABSA - Subtask 1: Dimensional Aspect Sentiment Regression (DimASR)\n",
    "======================================================================\n",
    "Task: Given Text + Aspect ‚Üí Predict Valence and Arousal (1.00-9.00)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì All imports loaded!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION\n",
      "============================================================\n",
      "Model: microsoft/deberta-v3-base\n",
      "Device: cpu\n",
      "Batch Size: 8\n",
      "Epochs: 5\n",
      "Max Length: 256\n",
      "Learning Rate: 2e-05\n",
      "Ensemble Seeds: [42, 123, 456]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths - UPDATE THESE TO YOUR PATHS\n",
    "    TRAIN_PATH = \"../data/trackA/subtask1/eng_laptop_train_alltasks.jsonl\"\n",
    "    DEV_PATH = \"../data/trackA/subtask1/eng_laptop_dev_task1.jsonl\"\n",
    "    \n",
    "    # Model Selection\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "    # Alternatives (comment/uncomment to try):\n",
    "    # MODEL_NAME = \"microsoft/deberta-v3-large\"  # Better performance, slower\n",
    "    # MODEL_NAME = \"roberta-large\"\n",
    "    \n",
    "    # Model Parameters\n",
    "    MAX_LEN = 256  # Increased from 128\n",
    "    DROPOUT = 0.2\n",
    "    HIDDEN_DIM = 256  # For intermediate layers\n",
    "    \n",
    "    # Training Parameters\n",
    "    BATCH_SIZE = 8  # Reduce to 4 if OOM (Out of Memory)\n",
    "    LR = 2e-5\n",
    "    EPOCHS = 5\n",
    "    WARMUP_RATIO = 0.1\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # Ensemble Settings\n",
    "    NUM_SEEDS = 3  # Train 3 models with different seeds\n",
    "    SEEDS = [42, 123, 456]\n",
    "    \n",
    "    # Hardware\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Outputs\n",
    "    OUTPUT_FILE = \"submission_task1_improved.jsonl\"\n",
    "    MODEL_SAVE_DIR = \"./saved_models\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create save directory\n",
    "import os\n",
    "os.makedirs(config.MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Max Length: {config.MAX_LEN}\")\n",
    "print(f\"Learning Rate: {config.LR}\")\n",
    "print(f\"Ensemble Seeds: {config.SEEDS}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Seed set to 42\n",
      "\n",
      "‚úì Utility functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"‚úì Seed set to {seed}\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    \"\"\"Load JSONL file\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def parse_va(va_str):\n",
    "    \"\"\"Parse 'V#A' format to (valence, arousal) floats\"\"\"\n",
    "    v, a = va_str.split(\"#\")\n",
    "    return float(v), float(a)\n",
    "\n",
    "def format_va(v, a):\n",
    "    \"\"\"Format valence and arousal to 'V#A' string\"\"\"\n",
    "    return f\"{v:.2f}#{a:.2f}\"\n",
    "\n",
    "# Set initial seed\n",
    "set_seed(config.SEEDS[0])\n",
    "\n",
    "print(\"\\n‚úì Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚úì Loaded 4076 training instances\n",
      "‚úì Loaded 200 dev instances\n",
      "\n",
      "============================================================\n",
      "TRAINING DATA SAMPLE\n",
      "============================================================\n",
      "{\n",
      "  \"ID\": \"laptop_quad_dev_1\",\n",
      "  \"Text\": \"this unit is ` ` pretty ` ` and stylish , so my high school daughter was attracted to it for that reason .\",\n",
      "  \"Quadruplet\": [\n",
      "    {\n",
      "      \"Aspect\": \"unit\",\n",
      "      \"Category\": \"LAPTOP#DESIGN_FEATURES\",\n",
      "      \"Opinion\": \"pretty\",\n",
      "      \"VA\": \"7.12#7.12\"\n",
      "    },\n",
      "    {\n",
      "      \"Aspect\": \"unit\",\n",
      "      \"Category\": \"LAPTOP#DESIGN_FEATURES\",\n",
      "      \"Opinion\": \"stylish\",\n",
      "      \"VA\": \"7.12#7.12\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "============================================================\n",
      "DEV DATA SAMPLE\n",
      "============================================================\n",
      "{\n",
      "  \"ID\": \"lap26_aspect_va_dev_1\",\n",
      "  \"Text\": \"The touchscreen works very well\",\n",
      "  \"Aspect\": [\n",
      "    \"touchscreen\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_raw = load_jsonl(config.TRAIN_PATH)\n",
    "dev_raw = load_jsonl(config.DEV_PATH)\n",
    "\n",
    "print(f\"‚úì Loaded {len(train_raw)} training instances\")\n",
    "print(f\"‚úì Loaded {len(dev_raw)} dev instances\")\n",
    "\n",
    "# Show example training data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING DATA SAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(train_raw[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "# Show example dev data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEV DATA SAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(dev_raw[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating samples...\n",
      "‚úì Created 5773 training samples\n",
      "‚úì Created 275 dev samples\n",
      "\n",
      "============================================================\n",
      "TRAINING DATA STATISTICS\n",
      "============================================================\n",
      "           valence     arousal\n",
      "count  5773.000000  5773.00000\n",
      "mean      5.936842     6.66797\n",
      "std       1.763164     1.03192\n",
      "min       1.000000     3.83000\n",
      "25%       4.380000     5.83000\n",
      "50%       6.620000     6.88000\n",
      "75%       7.380000     7.50000\n",
      "max       8.830000     8.83000\n",
      "\n",
      "üìä Sample distribution:\n",
      "                  id                                               text  \\\n",
      "0  laptop_quad_dev_1  this unit is ` ` pretty ` ` and stylish , so m...   \n",
      "1  laptop_quad_dev_1  this unit is ` ` pretty ` ` and stylish , so m...   \n",
      "2  laptop_quad_dev_2  for now i ' m okay with upping the experience ...   \n",
      "3  laptop_quad_dev_3  seems unlikely but whatever , i ' ll go with it .   \n",
      "4  laptop_quad_dev_4  this version has been my least favorite versio...   \n",
      "5  laptop_quad_dev_5        - biggest disappointment is the track pad .   \n",
      "6  laptop_quad_dev_6             should not of bought this chromebook .   \n",
      "7  laptop_quad_dev_7  after about 5 / 10 minutes of use the screen g...   \n",
      "8  laptop_quad_dev_8  i can not stand the trackpad or the keyboard k...   \n",
      "9  laptop_quad_dev_8  i can not stand the trackpad or the keyboard k...   \n",
      "\n",
      "          aspect  valence  arousal  \n",
      "0           unit     7.12     7.12  \n",
      "1           unit     7.12     7.12  \n",
      "2         device     5.50     5.25  \n",
      "3           NULL     5.00     5.12  \n",
      "4        version     3.30     6.60  \n",
      "5      track pad     2.50     6.00  \n",
      "6     chromebook     4.00     4.75  \n",
      "7         screen     2.00     7.50  \n",
      "8       trackpad     3.70     6.30  \n",
      "9  keyboard keys     3.70     6.30  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def create_task1_samples(raw_data, is_test=False):\n",
    "    \"\"\"\n",
    "    Convert raw JSONL data to samples for Task 1.\n",
    "    \n",
    "    Training format: {ID, Text, Quadruplet: [{Aspect, Category, Opinion, VA}]}\n",
    "    Test format: {ID, Text, Aspect: [list of aspects]}\n",
    "    \n",
    "    Output: List of {id, text, aspect, valence, arousal}\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for item in raw_data:\n",
    "        text = item[\"Text\"]\n",
    "        item_id = item[\"ID\"]\n",
    "        \n",
    "        if is_test:\n",
    "            # Dev/Test: Extract from Aspect list\n",
    "            for aspect in item[\"Aspect\"]:\n",
    "                samples.append({\n",
    "                    \"id\": item_id,\n",
    "                    \"text\": text,\n",
    "                    \"aspect\": aspect,\n",
    "                    \"valence\": 0.0,  # Placeholder\n",
    "                    \"arousal\": 0.0   # Placeholder\n",
    "                })\n",
    "        else:\n",
    "            # Training: Extract from Quadruplet list\n",
    "            for quad in item[\"Quadruplet\"]:\n",
    "                aspect = quad[\"Aspect\"]\n",
    "                v, a = parse_va(quad[\"VA\"])\n",
    "                samples.append({\n",
    "                    \"id\": item_id,\n",
    "                    \"text\": text,\n",
    "                    \"aspect\": aspect,\n",
    "                    \"valence\": v,\n",
    "                    \"arousal\": a\n",
    "                })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Create samples\n",
    "print(\"Creating samples...\")\n",
    "train_samples = create_task1_samples(train_raw, is_test=False)\n",
    "dev_samples = create_task1_samples(dev_raw, is_test=True)\n",
    "\n",
    "print(f\"‚úì Created {len(train_samples)} training samples\")\n",
    "print(f\"‚úì Created {len(dev_samples)} dev samples\")\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "dev_df = pd.DataFrame(dev_samples)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING DATA STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(train_df.describe())\n",
    "\n",
    "print(\"\\nüìä Sample distribution:\")\n",
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA SPLIT\n",
      "============================================================\n",
      "Training samples:   5195\n",
      "Validation samples: 578\n",
      "Dev/Test samples:   275\n",
      "============================================================\n",
      "\n",
      "üìä Valence Statistics (Training):\n",
      "count    5195.000000\n",
      "mean        5.932689\n",
      "std         1.768976\n",
      "min         1.000000\n",
      "25%         4.380000\n",
      "50%         6.620000\n",
      "75%         7.390000\n",
      "max         8.830000\n",
      "Name: valence, dtype: float64\n",
      "\n",
      "üìä Arousal Statistics (Training):\n",
      "count    5195.000000\n",
      "mean        6.670192\n",
      "std         1.034896\n",
      "min         3.830000\n",
      "25%         5.800000\n",
      "50%         6.880000\n",
      "75%         7.500000\n",
      "max         8.830000\n",
      "Name: arousal, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN-VALIDATION SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "# Split training data into train and validation (90/10)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples:   {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Dev/Test samples:   {len(dev_df)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show valence/arousal statistics\n",
    "print(\"\\nüìä Valence Statistics (Training):\")\n",
    "print(train_df['valence'].describe())\n",
    "\n",
    "print(\"\\nüìä Arousal Statistics (Training):\")\n",
    "print(train_df['arousal'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset class created!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class DimASRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Dimensional Aspect Sentiment Regression.\n",
    "    \n",
    "    Improvements:\n",
    "    - Two-sentence encoding: [CLS] text [SEP] aspect [SEP]\n",
    "    - This helps the model better attend to aspect-text relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=256):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.aspects = df[\"aspect\"].tolist()\n",
    "        self.valence = df[\"valence\"].tolist()\n",
    "        self.arousal = df[\"arousal\"].tolist()\n",
    "        self.ids = df[\"id\"].tolist()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        aspect = self.aspects[idx]\n",
    "        valence = self.valence[idx]\n",
    "        arousal = self.arousal[idx]\n",
    "        \n",
    "        # Encode as two sentences: [CLS] text [SEP] aspect [SEP]\n",
    "        # This is better than concatenation \"aspect: text\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            aspect,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor([valence, arousal], dtype=torch.float),\n",
    "            \"id\": self.ids[idx],\n",
    "            \"aspect\": aspect\n",
    "        }\n",
    "\n",
    "print(\"‚úì Dataset class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: microsoft/deberta-v3-base\n",
      "============================================================\n",
      "DATALOADERS CREATED\n",
      "============================================================\n",
      "Train batches: 650\n",
      "Val batches:   73\n",
      "Dev batches:   35\n",
      "============================================================\n",
      "\n",
      "üì¶ Sample batch shapes:\n",
      "  input_ids: torch.Size([8, 256])\n",
      "  attention_mask: torch.Size([8, 256])\n",
      "  labels: torch.Size([8, 2])\n",
      "\n",
      "üìù Encoded example:\n",
      "[CLS] no backlit keyboard[SEP] backlit keyboard[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
      "\n",
      "üéØ Target VA: [4.62 5.  ]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TOKENIZER & DATALOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Loading tokenizer: {config.MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DimASRDataset(train_df, tokenizer, config.MAX_LEN)\n",
    "val_dataset = DimASRDataset(val_df, tokenizer, config.MAX_LEN)\n",
    "dev_dataset = DimASRDataset(dev_df, tokenizer, config.MAX_LEN)\n",
    "\n",
    "# Create dataloaders\n",
    "# NOTE: num_workers=0 for Jupyter notebook compatibility\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Changed from 2 to 0 for Jupyter\n",
    "    pin_memory=True if config.DEVICE == \"cuda\" else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Changed from 2 to 0 for Jupyter\n",
    "    pin_memory=True if config.DEVICE == \"cuda\" else False\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Changed from 2 to 0 for Jupyter\n",
    "    pin_memory=True if config.DEVICE == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATALOADERS CREATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Dev batches:   {len(dev_loader)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test one batch\n",
    "batch = next(iter(train_loader))\n",
    "print(\"\\nüì¶ Sample batch shapes:\")\n",
    "print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  labels: {batch['labels'].shape}\")\n",
    "\n",
    "# Decode one example to verify encoding\n",
    "sample_idx = 0\n",
    "decoded = tokenizer.decode(batch['input_ids'][sample_idx], skip_special_tokens=False)\n",
    "print(f\"\\nüìù Encoded example:\\n{decoded}\")\n",
    "print(f\"\\nüéØ Target VA: {batch['labels'][sample_idx].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model architecture defined!\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "Architecture: microsoft/deberta-v3-base\n",
      "Total parameters: 184,095,490\n",
      "Trainable parameters: 184,095,490\n",
      "Device: cpu\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedVARegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Valence-Arousal Regressor\n",
    "    \n",
    "    Improvements:\n",
    "    1. Mean pooling instead of CLS (more stable)\n",
    "    2. Separate heads for Valence and Arousal\n",
    "    3. Additional hidden layer with LayerNorm\n",
    "    4. GELU activation (better than ReLU)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, dropout=0.2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Shared projection layer\n",
    "        self.shared_projection = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Separate heads for Valence and Arousal\n",
    "        self.valence_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.arousal_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def mean_pool(self, hidden_states, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling over all tokens (masked).\n",
    "        More stable than CLS token alone.\n",
    "        \"\"\"\n",
    "        # Expand mask to match hidden dimensions\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        \n",
    "        # Mask out padding tokens\n",
    "        masked_hidden = hidden_states * mask\n",
    "        \n",
    "        # Sum and normalize\n",
    "        summed = masked_hidden.sum(dim=1)\n",
    "        counted = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        \n",
    "        return summed / counted\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get encoder outputs\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Mean pooling\n",
    "        pooled = self.mean_pool(outputs.last_hidden_state, attention_mask)\n",
    "        \n",
    "        # Shared projection\n",
    "        shared = self.shared_projection(pooled)\n",
    "        \n",
    "        # Separate predictions\n",
    "        valence = self.valence_head(shared)\n",
    "        arousal = self.arousal_head(shared)\n",
    "        \n",
    "        # Concatenate (batch_size, 2)\n",
    "        output = torch.cat([valence, arousal], dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úì Model architecture defined!\")\n",
    "\n",
    "# Show model summary\n",
    "model = ImprovedVARegressor(\n",
    "    config.MODEL_NAME, \n",
    "    dropout=config.DROPOUT,\n",
    "    hidden_dim=config.HIDDEN_DIM\n",
    ").to(config.DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Architecture: {config.MODEL_NAME}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Device: {config.DEVICE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loss function created!\n",
      "Test loss value: 0.1087\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "class RobustVALoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Robust loss combining MSE and Huber loss.\n",
    "    \n",
    "    - MSE: Good for normal predictions\n",
    "    - Huber: Robust to outliers\n",
    "    - Combination provides stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.5, huber_delta=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.huber = nn.HuberLoss(delta=huber_delta)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        huber_loss = self.huber(pred, target)\n",
    "        return self.alpha * mse_loss + (1 - self.alpha) * huber_loss\n",
    "\n",
    "# Test the loss function\n",
    "criterion = RobustVALoss(alpha=0.5)\n",
    "\n",
    "# Test with dummy data\n",
    "dummy_pred = torch.tensor([[7.5, 6.8], [5.2, 5.5]])\n",
    "dummy_target = torch.tensor([[7.0, 7.0], [5.0, 5.0]])\n",
    "test_loss = criterion(dummy_pred, dummy_target)\n",
    "\n",
    "print(\"‚úì Loss function created!\")\n",
    "print(f\"Test loss value: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training and evaluation functions ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # RMSE (official metric)\n",
    "    rmse = np.sqrt(np.mean((all_preds - all_labels) ** 2))\n",
    "    \n",
    "    # Pearson correlation\n",
    "    pcc_valence = pearsonr(all_preds[:, 0], all_labels[:, 0])[0]\n",
    "    pcc_arousal = pearsonr(all_preds[:, 1], all_labels[:, 1])[0]\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rmse\": rmse,\n",
    "        \"pcc_valence\": pcc_valence,\n",
    "        \"pcc_arousal\": pcc_arousal,\n",
    "        \"predictions\": all_preds,\n",
    "        \"labels\": all_labels\n",
    "    }\n",
    "\n",
    "print(\"‚úì Training and evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training loop ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(train_loader, val_loader, config, seed=42):\n",
    "    \"\"\"Complete training loop for one model\"\"\"\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ImprovedVARegressor(\n",
    "        config.MODEL_NAME,\n",
    "        dropout=config.DROPOUT,\n",
    "        hidden_dim=config.HIDDEN_DIM\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RobustVALoss(alpha=0.5)\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.LR,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    num_training_steps = len(train_loader) * config.EPOCHS\n",
    "    num_warmup_steps = int(config.WARMUP_RATIO * num_training_steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Training tracking\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    history = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üöÄ TRAINING MODEL (Seed: {seed})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_metrics = evaluate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        # Log metrics\n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_metrics[\"loss\"],\n",
    "            \"val_rmse\": val_metrics[\"rmse\"],\n",
    "            \"val_pcc_v\": val_metrics[\"pcc_valence\"],\n",
    "            \"val_pcc_a\": val_metrics[\"pcc_arousal\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss:   {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val RMSE:   {val_metrics['rmse']:.4f} ‚≠ê\")\n",
    "        print(f\"Val PCC-V:  {val_metrics['pcc_valence']:.4f}\")\n",
    "        print(f\"Val PCC-A:  {val_metrics['pcc_arousal']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics[\"rmse\"] < best_val_rmse:\n",
    "            best_val_rmse = val_metrics[\"rmse\"]\n",
    "            model_path = f\"{config.MODEL_SAVE_DIR}/best_model_seed{seed}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"üíæ Best model saved! (RMSE: {best_val_rmse:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úÖ TRAINING COMPLETE (Seed: {seed})\")\n",
    "    print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, best_val_rmse, history\n",
    "\n",
    "print(\"‚úì Training loop ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Seed set to 42\n",
      "\n",
      "============================================================\n",
      "üöÄ TRAINING MODEL (Seed: 42)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5640\n",
      "Val Loss:   2.7165\n",
      "Val RMSE:   2.0304 ‚≠ê\n",
      "Val PCC-V:  0.0803\n",
      "Val PCC-A:  -0.0360\n",
      "üíæ Best model saved! (RMSE: 2.0304)\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5757\n",
      "Val Loss:   1.6684\n",
      "Val RMSE:   1.5612 ‚≠ê\n",
      "Val PCC-V:  0.0150\n",
      "Val PCC-A:  0.0082\n",
      "üíæ Best model saved! (RMSE: 1.5612)\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8375\n",
      "Val Loss:   1.4089\n",
      "Val RMSE:   1.4306 ‚≠ê\n",
      "Val PCC-V:  0.0050\n",
      "Val PCC-A:  -0.0213\n",
      "üíæ Best model saved! (RMSE: 1.4306)\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6561\n",
      "Val Loss:   1.3592\n",
      "Val RMSE:   1.4071 ‚≠ê\n",
      "Val PCC-V:  0.0045\n",
      "Val PCC-A:  -0.0191\n",
      "üíæ Best model saved! (RMSE: 1.4071)\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6129\n",
      "Val Loss:   1.3515\n",
      "Val RMSE:   1.4040 ‚≠ê\n",
      "Val PCC-V:  0.0053\n",
      "Val PCC-A:  -0.0206\n",
      "üíæ Best model saved! (RMSE: 1.4040)\n",
      "\n",
      "============================================================\n",
      "‚úÖ TRAINING COMPLETE (Seed: 42)\n",
      "Best Validation RMSE: 1.4040\n",
      "============================================================\n",
      "\n",
      "üìä Training History:\n",
      "   epoch  train_loss  val_loss  val_rmse  val_pcc_v  val_pcc_a\n",
      "0      1    7.563998  2.716471  2.030386   0.080266  -0.035967\n",
      "1      2    2.575711  1.668426  1.561201   0.015034   0.008206\n",
      "2      3    1.837546  1.408887  1.430553   0.005023  -0.021291\n",
      "3      4    1.656081  1.359227  1.407104   0.004497  -0.019103\n",
      "4      5    1.612878  1.351523  1.404049   0.005339  -0.020636\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAIN FIRST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Train with first seed\n",
    "model, best_rmse, history = train_model(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    config,\n",
    "    seed=config.SEEDS[0]\n",
    ")\n",
    "\n",
    "# Show training history\n",
    "history_df = pd.DataFrame(history)\n",
    "print(\"\\nüìä Training History:\")\n",
    "print(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç DIAGNOSTIC ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä Prediction Statistics:\n",
      "Valence - Pred Mean: 5.89, Std: 0.00\n",
      "Valence - True Mean: 5.97, Std: 1.71\n",
      "Arousal - Pred Mean: 6.58, Std: 0.00\n",
      "Arousal - True Mean: 6.65, Std: 1.00\n",
      "\n",
      "üìã Sample Predictions (First 20):\n",
      "Pred Valence | True Valence | Pred Arousal | True Arousal\n",
      "------------------------------------------------------------\n",
      "     5.89    |      7.00    |      6.58    |      7.00\n",
      "     5.89    |      7.17    |      6.58    |      6.83\n",
      "     5.89    |      8.12    |      6.58    |      8.25\n",
      "     5.89    |      7.25    |      6.58    |      7.50\n",
      "     5.89    |      5.00    |      6.58    |      5.00\n",
      "     5.89    |      6.75    |      6.58    |      6.50\n",
      "     5.89    |      3.30    |      6.58    |      6.30\n",
      "     5.89    |      1.83    |      6.58    |      8.00\n",
      "     5.89    |      4.25    |      6.58    |      4.50\n",
      "     5.89    |      7.50    |      6.58    |      7.67\n",
      "     5.89    |      7.50    |      6.58    |      7.83\n",
      "     5.89    |      7.62    |      6.58    |      6.12\n",
      "     5.89    |      7.50    |      6.58    |      7.33\n",
      "     5.89    |      7.00    |      6.58    |      6.88\n",
      "     5.89    |      7.50    |      6.58    |      7.33\n",
      "     5.89    |      8.00    |      6.58    |      8.00\n",
      "     5.89    |      6.75    |      6.58    |      7.00\n",
      "     5.89    |      7.25    |      6.58    |      7.50\n",
      "     5.89    |      8.00    |      6.58    |      8.25\n",
      "     5.89    |      3.62    |      6.58    |      7.25\n",
      "\n",
      "‚ö†Ô∏è  Issues Detected:\n",
      "‚ùå Valence predictions have very low variance (model might be stuck)\n",
      "‚ùå Arousal predictions have very low variance (model might be stuck)\n",
      "\n",
      "üìà Prediction Range:\n",
      "Valence: [5.89, 5.89]\n",
      "Arousal: [6.58, 6.58]\n",
      "Expected: [1.00, 9.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: Check Predictions vs Ground Truth\n",
    "# ============================================================================\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(f\"{config.MODEL_SAVE_DIR}/best_model_seed42.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions on validation set\n",
    "val_metrics = evaluate(model, val_loader, criterion, config.DEVICE)\n",
    "\n",
    "preds = val_metrics[\"predictions\"]\n",
    "labels = val_metrics[\"labels\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç DIAGNOSTIC ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check prediction statistics\n",
    "print(\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"Valence - Pred Mean: {preds[:, 0].mean():.2f}, Std: {preds[:, 0].std():.2f}\")\n",
    "print(f\"Valence - True Mean: {labels[:, 0].mean():.2f}, Std: {labels[:, 0].std():.2f}\")\n",
    "print(f\"Arousal - Pred Mean: {preds[:, 1].mean():.2f}, Std: {preds[:, 1].std():.2f}\")\n",
    "print(f\"Arousal - True Mean: {labels[:, 1].mean():.2f}, Std: {labels[:, 1].std():.2f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüìã Sample Predictions (First 20):\")\n",
    "print(\"Pred Valence | True Valence | Pred Arousal | True Arousal\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(20):\n",
    "    print(f\"   {preds[i, 0]:6.2f}    |    {labels[i, 0]:6.2f}    |    {preds[i, 1]:6.2f}    |    {labels[i, 1]:6.2f}\")\n",
    "\n",
    "# Check if predictions are stuck\n",
    "print(\"\\n‚ö†Ô∏è  Issues Detected:\")\n",
    "if preds[:, 0].std() < 0.5:\n",
    "    print(\"‚ùå Valence predictions have very low variance (model might be stuck)\")\n",
    "if preds[:, 1].std() < 0.5:\n",
    "    print(\"‚ùå Arousal predictions have very low variance (model might be stuck)\")\n",
    "\n",
    "# Check actual distribution\n",
    "print(\"\\nüìà Prediction Range:\")\n",
    "print(f\"Valence: [{preds[:, 0].min():.2f}, {preds[:, 0].max():.2f}]\")\n",
    "print(f\"Arousal: [{preds[:, 1].min():.2f}, {preds[:, 1].max():.2f}]\")\n",
    "print(f\"Expected: [1.00, 9.00]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîß RAW MODEL OUTPUTS (Before scaling)\n",
      "============================================================\n",
      "Raw output range: [5.89, 6.58]\n",
      "Expected range: [1.00, 9.00]\n",
      "\n",
      "First 10 raw predictions:\n",
      "tensor([[5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813],\n",
      "        [5.8943, 6.5813]])\n",
      "\n",
      "First 10 true labels:\n",
      "tensor([[7.0000, 7.0000],\n",
      "        [7.1700, 6.8300],\n",
      "        [8.1200, 8.2500],\n",
      "        [7.2500, 7.5000],\n",
      "        [5.0000, 5.0000],\n",
      "        [6.7500, 6.5000],\n",
      "        [3.3000, 6.3000],\n",
      "        [1.8300, 8.0000]])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK: Are outputs in wrong range?\n",
    "# ============================================================================\n",
    "\n",
    "# Get raw model outputs (before any clipping)\n",
    "model.eval()\n",
    "sample_batch = next(iter(val_loader))\n",
    "input_ids = sample_batch[\"input_ids\"].to(config.DEVICE)\n",
    "attention_mask = sample_batch[\"attention_mask\"].to(config.DEVICE)\n",
    "labels = sample_batch[\"labels\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_outputs = model(input_ids, attention_mask).cpu()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß RAW MODEL OUTPUTS (Before scaling)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Raw output range: [{raw_outputs.min():.2f}, {raw_outputs.max():.2f}]\")\n",
    "print(f\"Expected range: [1.00, 9.00]\")\n",
    "print(f\"\\nFirst 10 raw predictions:\")\n",
    "print(raw_outputs[:10])\n",
    "print(f\"\\nFirst 10 true labels:\")\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Improved Model V2 with output scaling created!\n",
      "\n",
      "‚úì V2 Model output range: [3.98, 6.24]\n",
      "‚úì Expected range: [1.00, 9.00]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED MODEL V2 - WITH OUTPUT SCALING\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedVARegressorV2(nn.Module):\n",
    "    \"\"\"\n",
    "    V2: Fixed version with proper output scaling to [1, 9] range\n",
    "    \n",
    "    Key fix: sigmoid(output) * 8 + 1 ‚Üí maps to [1, 9]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, dropout=0.2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained encoder\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Shared projection layer\n",
    "        self.shared_projection = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Separate heads for Valence and Arousal\n",
    "        self.valence_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.arousal_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def mean_pool(self, hidden_states, attention_mask):\n",
    "        \"\"\"Mean pooling over all tokens (masked)\"\"\"\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        masked_hidden = hidden_states * mask\n",
    "        summed = masked_hidden.sum(dim=1)\n",
    "        counted = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed / counted\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get encoder outputs\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Mean pooling\n",
    "        pooled = self.mean_pool(outputs.last_hidden_state, attention_mask)\n",
    "        \n",
    "        # Shared projection\n",
    "        shared = self.shared_projection(pooled)\n",
    "        \n",
    "        # Separate predictions\n",
    "        valence = self.valence_head(shared)\n",
    "        arousal = self.arousal_head(shared)\n",
    "        \n",
    "        # Concatenate\n",
    "        output = torch.cat([valence, arousal], dim=1)\n",
    "        \n",
    "        # üî• KEY FIX: Scale outputs to [1, 9] range\n",
    "        # sigmoid maps to [0, 1], then scale to [1, 9]\n",
    "        output = torch.sigmoid(output) * 8.0 + 1.0\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úì Improved Model V2 with output scaling created!\")\n",
    "\n",
    "# Test the new model\n",
    "model_v2_test = ImprovedVARegressorV2(\n",
    "    config.MODEL_NAME,\n",
    "    dropout=config.DROPOUT,\n",
    "    hidden_dim=config.HIDDEN_DIM\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# Test output range\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    test_output = model_v2_test(\n",
    "        test_batch[\"input_ids\"].to(config.DEVICE),\n",
    "        test_batch[\"attention_mask\"].to(config.DEVICE)\n",
    "    )\n",
    "    print(f\"\\n‚úì V2 Model output range: [{test_output.min():.2f}, {test_output.max():.2f}]\")\n",
    "    print(f\"‚úì Expected range: [1.00, 9.00]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Seed set to 42\n",
      "\n",
      "============================================================\n",
      "üöÄ RETRAINING WITH V2 MODEL (Seed: 42)\n",
      "============================================================\n",
      "\n",
      "Epoch 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8171\n",
      "Val Loss:   0.4893\n",
      "Val RMSE:   0.8364 ‚≠ê\n",
      "Val PCC-V:  0.8687\n",
      "Val PCC-A:  0.7008\n",
      "üíæ Best model saved! (RMSE: 0.8364)\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4392\n",
      "Val Loss:   0.4675\n",
      "Val RMSE:   0.8189 ‚≠ê\n",
      "Val PCC-V:  0.8805\n",
      "Val PCC-A:  0.6992\n",
      "üíæ Best model saved! (RMSE: 0.8189)\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3253\n",
      "Val Loss:   0.4338\n",
      "Val RMSE:   0.7898 ‚≠ê\n",
      "Val PCC-V:  0.8868\n",
      "Val PCC-A:  0.7143\n",
      "üíæ Best model saved! (RMSE: 0.7898)\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2552\n",
      "Val Loss:   0.4278\n",
      "Val RMSE:   0.7846 ‚≠ê\n",
      "Val PCC-V:  0.8834\n",
      "Val PCC-A:  0.7249\n",
      "üíæ Best model saved! (RMSE: 0.7846)\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2101\n",
      "Val Loss:   0.4467\n",
      "Val RMSE:   0.8020 ‚≠ê\n",
      "Val PCC-V:  0.8835\n",
      "Val PCC-A:  0.7175\n",
      "\n",
      "============================================================\n",
      "‚úÖ RETRAINING COMPLETE\n",
      "Best Validation RMSE: 0.7846\n",
      "============================================================\n",
      "\n",
      "üìä Training History V2:\n",
      "   epoch  train_loss  val_loss  val_rmse  val_pcc_v  val_pcc_a\n",
      "0      1    0.817129  0.489270  0.836363   0.868731   0.700768\n",
      "1      2    0.439178  0.467470  0.818895   0.880543   0.699232\n",
      "2      3    0.325253  0.433773  0.789757   0.886777   0.714288\n",
      "3      4    0.255246  0.427774  0.784615   0.883364   0.724859\n",
      "4      5    0.210080  0.446668  0.801987   0.883485   0.717456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN WITH V2 MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def train_model_v2(train_loader, val_loader, config, seed=42):\n",
    "    \"\"\"Training loop with V2 model (includes output scaling)\"\"\"\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Use V2 model with sigmoid scaling\n",
    "    model = ImprovedVARegressorV2(\n",
    "        config.MODEL_NAME,\n",
    "        dropout=config.DROPOUT,\n",
    "        hidden_dim=config.HIDDEN_DIM\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    criterion = RobustVALoss(alpha=0.5)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.LR,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    num_training_steps = len(train_loader) * config.EPOCHS\n",
    "    num_warmup_steps = int(config.WARMUP_RATIO * num_training_steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    best_val_rmse = float(\"inf\")\n",
    "    history = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"üöÄ RETRAINING WITH V2 MODEL (Seed: {seed})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, scheduler, criterion, config.DEVICE\n",
    "        )\n",
    "        \n",
    "        val_metrics = evaluate(model, val_loader, criterion, config.DEVICE)\n",
    "        \n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_metrics[\"loss\"],\n",
    "            \"val_rmse\": val_metrics[\"rmse\"],\n",
    "            \"val_pcc_v\": val_metrics[\"pcc_valence\"],\n",
    "            \"val_pcc_a\": val_metrics[\"pcc_arousal\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss:   {val_metrics['loss']:.4f}\")\n",
    "        print(f\"Val RMSE:   {val_metrics['rmse']:.4f} ‚≠ê\")\n",
    "        print(f\"Val PCC-V:  {val_metrics['pcc_valence']:.4f}\")\n",
    "        print(f\"Val PCC-A:  {val_metrics['pcc_arousal']:.4f}\")\n",
    "        \n",
    "        if val_metrics[\"rmse\"] < best_val_rmse:\n",
    "            best_val_rmse = val_metrics[\"rmse\"]\n",
    "            model_path = f\"{config.MODEL_SAVE_DIR}/best_model_v2_seed{seed}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"üíæ Best model saved! (RMSE: {best_val_rmse:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úÖ RETRAINING COMPLETE\")\n",
    "    print(f\"Best Validation RMSE: {best_val_rmse:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, best_val_rmse, history\n",
    "\n",
    "# Train V2 model\n",
    "model_v2, best_rmse_v2, history_v2 = train_model_v2(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    config,\n",
    "    seed=config.SEEDS[0]\n",
    ")\n",
    "\n",
    "# Show history\n",
    "print(\"\\nüìä Training History V2:\")\n",
    "print(pd.DataFrame(history_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîÆ GENERATING PREDICTIONS ON DEV SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:32<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Predictions generated!\n",
      "Total predictions: 275\n",
      "\n",
      "üìä Prediction Statistics:\n",
      "Valence - Mean: 6.69, Std: 1.51\n",
      "Valence - Range: [2.26, 8.12]\n",
      "Arousal - Mean: 7.08, Std: 0.78\n",
      "Arousal - Range: [4.97, 8.17]\n",
      "\n",
      "üìã Sample Predictions:\n",
      "                        id                         aspect  pred_valence  \\\n",
      "0    lap26_aspect_va_dev_1                    touchscreen      6.945566   \n",
      "1    lap26_aspect_va_dev_2                             HP      2.403815   \n",
      "2    lap26_aspect_va_dev_3                       keyboard      7.005983   \n",
      "3    lap26_aspect_va_dev_4                    screen size      6.907866   \n",
      "4    lap26_aspect_va_dev_5                         Lenovo      7.790746   \n",
      "5    lap26_aspect_va_dev_6                          sound      7.626075   \n",
      "6    lap26_aspect_va_dev_7                        quality      8.026880   \n",
      "7    lap26_aspect_va_dev_8             on screen keyboard      7.182424   \n",
      "8    lap26_aspect_va_dev_9                         laptop      7.908240   \n",
      "9   lap26_aspect_va_dev_10  functionality of the trackpad      7.349179   \n",
      "10  lap26_aspect_va_dev_11                     new Lenovo      7.593071   \n",
      "11  lap26_aspect_va_dev_12                   touch screen      7.534599   \n",
      "12  lap26_aspect_va_dev_12                         screen      7.506182   \n",
      "13  lap26_aspect_va_dev_13                        Dell 13      6.595482   \n",
      "14  lap26_aspect_va_dev_13                       keyboard      4.371084   \n",
      "\n",
      "    pred_arousal  \n",
      "0       6.866993  \n",
      "1       7.078559  \n",
      "2       6.901433  \n",
      "3       6.796800  \n",
      "4       7.793088  \n",
      "5       7.675780  \n",
      "6       8.068931  \n",
      "7       7.118170  \n",
      "8       8.009718  \n",
      "9       7.371418  \n",
      "10      7.587709  \n",
      "11      7.517452  \n",
      "12      7.471412  \n",
      "13      6.669798  \n",
      "14      5.528967  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICT ON DEV/TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîÆ GENERATING PREDICTIONS ON DEV SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best V2 model\n",
    "model_v2.load_state_dict(torch.load(f\"{config.MODEL_SAVE_DIR}/best_model_v2_seed42.pt\"))\n",
    "model_v2.eval()\n",
    "\n",
    "# Predict function\n",
    "def predict_on_loader(model, loader, device):\n",
    "    \"\"\"Generate predictions for a dataloader\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_preds)\n",
    "\n",
    "# Generate predictions\n",
    "dev_preds = predict_on_loader(model_v2, dev_loader, config.DEVICE)\n",
    "\n",
    "# Clip to valid range [1.0, 9.0] (safety check)\n",
    "dev_preds = np.clip(dev_preds, 1.0, 9.0)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "dev_df[\"pred_valence\"] = dev_preds[:, 0]\n",
    "dev_df[\"pred_arousal\"] = dev_preds[:, 1]\n",
    "\n",
    "print(\"\\n‚úÖ Predictions generated!\")\n",
    "print(f\"Total predictions: {len(dev_preds)}\")\n",
    "\n",
    "# Show prediction statistics\n",
    "print(\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"Valence - Mean: {dev_preds[:, 0].mean():.2f}, Std: {dev_preds[:, 0].std():.2f}\")\n",
    "print(f\"Valence - Range: [{dev_preds[:, 0].min():.2f}, {dev_preds[:, 0].max():.2f}]\")\n",
    "print(f\"Arousal - Mean: {dev_preds[:, 1].mean():.2f}, Std: {dev_preds[:, 1].std():.2f}\")\n",
    "print(f\"Arousal - Range: [{dev_preds[:, 1].min():.2f}, {dev_preds[:, 1].max():.2f}]\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüìã Sample Predictions:\")\n",
    "print(dev_df[[\"id\", \"aspect\", \"pred_valence\", \"pred_arousal\"]].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üíæ CREATING SUBMISSION FILE\n",
      "============================================================\n",
      "‚úÖ Submission file saved: submission_task1_improved.jsonl\n",
      "üìù Total records: 200\n",
      "üìù Total predictions: 275\n",
      "\n",
      "============================================================\n",
      "üìÑ SAMPLE SUBMISSION LINES\n",
      "============================================================\n",
      "\n",
      "1. ID: lap26_aspect_va_dev_1\n",
      "   - touchscreen: 6.95#6.87\n",
      "\n",
      "2. ID: lap26_aspect_va_dev_10\n",
      "   - functionality of the trackpad: 7.35#7.37\n",
      "\n",
      "3. ID: lap26_aspect_va_dev_100\n",
      "   - convertible aspect: 6.56#6.48\n",
      "   - touchscreen: 6.72#6.67\n",
      "\n",
      "4. ID: lap26_aspect_va_dev_101\n",
      "   - quality of every app: 7.38#7.36\n",
      "   - double screen: 7.30#7.26\n",
      "\n",
      "5. ID: lap26_aspect_va_dev_102\n",
      "   - Battery: 7.57#7.55\n",
      "\n",
      "============================================================\n",
      "‚úÖ SUBTASK 1 COMPLETE!\n",
      "============================================================\n",
      "üìÅ Submission file: submission_task1_improved.jsonl\n",
      "üéØ Expected RMSE: ~0.78-0.82\n",
      "üèÜ Ready to upload to Codabench!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE SUBMISSION FILE\n",
    "# ============================================================================\n",
    "\n",
    "def save_submission(df, output_path):\n",
    "    \"\"\"\n",
    "    Save predictions in the required submission format.\n",
    "    \n",
    "    Format:\n",
    "    {\n",
    "      \"ID\": \"lap26_aspect_va_dev_1\",\n",
    "      \"Aspect_VA\": [\n",
    "        {\"Aspect\": \"touchscreen\", \"VA\": \"7.95#7.61\"},\n",
    "        {\"Aspect\": \"keyboard\", \"VA\": \"6.50#6.80\"}\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Sort by ID to maintain order\n",
    "    df_sorted = df.sort_values(by=\"id\")\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Group by ID (since each ID can have multiple aspects)\n",
    "        for gid, group in df_sorted.groupby(\"id\"):\n",
    "            record = {\n",
    "                \"ID\": gid,\n",
    "                \"Aspect_VA\": []\n",
    "            }\n",
    "            \n",
    "            # Add each aspect-VA pair\n",
    "            for _, row in group.iterrows():\n",
    "                record[\"Aspect_VA\"].append({\n",
    "                    \"Aspect\": row[\"aspect\"],\n",
    "                    \"VA\": f\"{row['pred_valence']:.2f}#{row['pred_arousal']:.2f}\"\n",
    "                })\n",
    "            \n",
    "            # Write as JSON line\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Submission file saved: {output_path}\")\n",
    "    print(f\"üìù Total records: {df_sorted['id'].nunique()}\")\n",
    "    print(f\"üìù Total predictions: {len(df_sorted)}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üíæ CREATING SUBMISSION FILE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate submission\n",
    "save_submission(dev_df, config.OUTPUT_FILE)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìÑ SAMPLE SUBMISSION LINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show first 5 submission lines\n",
    "with open(config.OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            data = json.loads(line)\n",
    "            print(f\"\\n{i+1}. ID: {data['ID']}\")\n",
    "            for aspect_va in data['Aspect_VA']:\n",
    "                print(f\"   - {aspect_va['Aspect']}: {aspect_va['VA']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ SUBTASK 1 COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Submission file: {config.OUTPUT_FILE}\")\n",
    "print(f\"üéØ Expected RMSE: ~0.78-0.82\")\n",
    "print(f\"üèÜ Ready to upload to Codabench!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING SUBMISSION FORMAT\n",
      "\n",
      "‚úì Total records in submission: 200\n",
      "‚úÖ SUBMISSION FORMAT IS VALID!\n",
      "\n",
      "üìä Submission Statistics:\n",
      "  - Total IDs: 200\n",
      "  - Total Aspects: 275\n",
      "  - Avg aspects per ID: 1.38\n",
      "\n",
      "  Valence Stats:\n",
      "    Mean: 6.69, Std: 1.51\n",
      "    Range: [2.26, 8.12]\n",
      "\n",
      "  Arousal Stats:\n",
      "    Mean: 7.08, Std: 0.78\n",
      "    Range: [4.97, 8.17]\n",
      "\n",
      "üéâ Ready to submit to Codabench!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFY SUBMISSION FORMAT (OPTIONAL)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç VERIFYING SUBMISSION FORMAT\\n\")\n",
    "\n",
    "# Load submission file\n",
    "with open(config.OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    submission_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"‚úì Total records in submission: {len(submission_data)}\")\n",
    "\n",
    "# Check format\n",
    "errors = []\n",
    "for i, record in enumerate(submission_data):\n",
    "    # Check required fields\n",
    "    if \"ID\" not in record:\n",
    "        errors.append(f\"Line {i+1}: Missing 'ID' field\")\n",
    "    if \"Aspect_VA\" not in record:\n",
    "        errors.append(f\"Line {i+1}: Missing 'Aspect_VA' field\")\n",
    "    else:\n",
    "        # Check Aspect_VA format\n",
    "        for j, aspect_va in enumerate(record[\"Aspect_VA\"]):\n",
    "            if \"Aspect\" not in aspect_va:\n",
    "                errors.append(f\"Line {i+1}, Aspect {j+1}: Missing 'Aspect' field\")\n",
    "            if \"VA\" not in aspect_va:\n",
    "                errors.append(f\"Line {i+1}, Aspect {j+1}: Missing 'VA' field\")\n",
    "            else:\n",
    "                # Check VA format (should be \"V#A\")\n",
    "                va_str = aspect_va[\"VA\"]\n",
    "                if \"#\" not in va_str:\n",
    "                    errors.append(f\"Line {i+1}, Aspect {j+1}: Invalid VA format (missing #)\")\n",
    "                else:\n",
    "                    try:\n",
    "                        v, a = va_str.split(\"#\")\n",
    "                        v_float = float(v)\n",
    "                        a_float = float(a)\n",
    "                        \n",
    "                        # Check range [1.0, 9.0]\n",
    "                        if not (1.0 <= v_float <= 9.0):\n",
    "                            errors.append(f\"Line {i+1}: Valence {v_float} out of range [1.0, 9.0]\")\n",
    "                        if not (1.0 <= a_float <= 9.0):\n",
    "                            errors.append(f\"Line {i+1}: Arousal {a_float} out of range [1.0, 9.0]\")\n",
    "                    except ValueError:\n",
    "                        errors.append(f\"Line {i+1}: Invalid VA values (not numeric)\")\n",
    "\n",
    "# Report results\n",
    "if errors:\n",
    "    print(\"‚ùå ERRORS FOUND:\")\n",
    "    for error in errors[:10]:  # Show first 10 errors\n",
    "        print(f\"  - {error}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"  ... and {len(errors) - 10} more errors\")\n",
    "else:\n",
    "    print(\"‚úÖ SUBMISSION FORMAT IS VALID!\")\n",
    "    print(\"\\nüìä Submission Statistics:\")\n",
    "    \n",
    "    total_aspects = sum(len(record[\"Aspect_VA\"]) for record in submission_data)\n",
    "    print(f\"  - Total IDs: {len(submission_data)}\")\n",
    "    print(f\"  - Total Aspects: {total_aspects}\")\n",
    "    print(f\"  - Avg aspects per ID: {total_aspects / len(submission_data):.2f}\")\n",
    "    \n",
    "    # Get VA statistics\n",
    "    all_v = []\n",
    "    all_a = []\n",
    "    for record in submission_data:\n",
    "        for aspect_va in record[\"Aspect_VA\"]:\n",
    "            v, a = aspect_va[\"VA\"].split(\"#\")\n",
    "            all_v.append(float(v))\n",
    "            all_a.append(float(a))\n",
    "    \n",
    "    print(f\"\\n  Valence Stats:\")\n",
    "    print(f\"    Mean: {np.mean(all_v):.2f}, Std: {np.std(all_v):.2f}\")\n",
    "    print(f\"    Range: [{np.min(all_v):.2f}, {np.max(all_v):.2f}]\")\n",
    "    print(f\"\\n  Arousal Stats:\")\n",
    "    print(f\"    Mean: {np.mean(all_a):.2f}, Std: {np.std(all_a):.2f}\")\n",
    "    print(f\"    Range: [{np.min(all_a):.2f}, {np.max(all_a):.2f}]\")\n",
    "    \n",
    "    print(\"\\nüéâ Ready to submit to Codabench!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimabsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
